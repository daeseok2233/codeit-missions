{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyONys+8QL2rZnPUVqkptrW7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"42bd0750cf844e389b40571ecee991a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b317af5801b44f6187197d378e271031","IPY_MODEL_d452fd33e5fe48b3a3d007c6b426500b","IPY_MODEL_a4dbdae85fda4b21b09b70879dc6d18a"],"layout":"IPY_MODEL_82d766883d014263936d46f77c2546a1"}},"b317af5801b44f6187197d378e271031":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fce2c5e1757480dacfeeae64cb4fb48","placeholder":"​","style":"IPY_MODEL_dbaa3c19e68e4401a8b6e5f26bf5093f","value":"Calibrating (CPU):  95%"}},"d452fd33e5fe48b3a3d007c6b426500b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_83c0970dce6a461eabcfb8cd619397d8","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b7e2d2077824285ad8b7969b1157f28","value":19}},"a4dbdae85fda4b21b09b70879dc6d18a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_769b1129e98b48f29450031b0e07bae9","placeholder":"​","style":"IPY_MODEL_e496e257d1b748b99d79560e72ae7070","value":" 19/20 [00:12&lt;00:00,  1.45it/s]"}},"82d766883d014263936d46f77c2546a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fce2c5e1757480dacfeeae64cb4fb48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbaa3c19e68e4401a8b6e5f26bf5093f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83c0970dce6a461eabcfb8cd619397d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b7e2d2077824285ad8b7969b1157f28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"769b1129e98b48f29450031b0e07bae9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e496e257d1b748b99d79560e72ae7070":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## 데이터셋\n","\n","- CIFAR 10\n","- train 데이터를 train 과 val로 split(8:2)\n","- 이미지 : 32 -> 128 resize\n","- RandomHorizontalFlip(p = 0.5)\n","- 정규화\n","- batch_size = 32 / 32"],"metadata":{"id":"Favzo6VtI43E"}},{"cell_type":"code","source":["# modeling.ipynb : CIFAR-10 train만 받아서 train/val로 사용 (8:2, 계층분할)\n","\n","import numpy as np\n","import torch, torchvision as tv, torchvision.transforms as T\n","from torchvision.transforms import InterpolationMode\n","from torch.utils.data import Subset, DataLoader\n","\n","# --------------------\n","# 기본 설정 & 재현성\n","# --------------------\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","SEED, VAL_RATIO = 42, 0.2\n","\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# DataLoader shuffle의 재현성 보장용\n","g = torch.Generator()\n","g.manual_seed(SEED)\n","\n","# --------------------\n","# 정규화 통계\n","# - ImageNet 사전학습 모델 사용 시: IMAGENET_STATS\n","# - scratch 학습 시: CIFAR10_STATS\n","# --------------------\n","IMAGENET_MEAN, IMAGENET_STD = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n","CIFAR10_MEAN,  CIFAR10_STD  = [0.4914,0.4822,0.4465], [0.2023,0.1994,0.2010]\n","\n","USE_IMAGENET_NORM = True  # ← VGG/ResNet 등 ImageNet pretrained면 True\n","MEAN = IMAGENET_MEAN if USE_IMAGENET_NORM else CIFAR10_MEAN\n","STD  = IMAGENET_STD  if USE_IMAGENET_NORM else CIFAR10_STD\n","\n","# --------------------\n","# Transforms\n","# --------------------\n","transform_train = T.Compose([\n","    T.Resize(128, interpolation=InterpolationMode.BILINEAR, antialias=True),\n","    T.RandomHorizontalFlip(p=0.5),\n","    T.ToTensor(),\n","    T.Normalize(MEAN, STD),\n","])\n","\n","transform_val = T.Compose([\n","    T.Resize(128, interpolation=InterpolationMode.BILINEAR, antialias=True),\n","    T.ToTensor(),\n","    T.Normalize(MEAN, STD),\n","])\n","\n","# --------------------\n","# 데이터 로딩 (train만 두 번 생성: 증강/비증강 분리용)\n","# --------------------\n","train_aug = tv.datasets.CIFAR10(root=\"./data\", train=True, download=True,  transform=transform_train)\n","val_eval  = tv.datasets.CIFAR10(root=\"./data\", train=True, download=False, transform=transform_val)\n","\n","# --------------------\n","# 계층(클래스별) 분할\n","# --------------------\n","labels = np.array(train_aug.targets)             # 길이 50,000\n","train_idx, val_idx = [], []\n","rng = np.random.default_rng(SEED)\n","\n","for c in np.unique(labels):\n","    idx = np.where(labels == c)[0]\n","    rng.shuffle(idx)\n","    n_val = max(1, int(round(len(idx) * VAL_RATIO)))\n","    val_idx.extend(idx[:n_val])\n","    train_idx.extend(idx[n_val:])\n","\n","# Subset 구성: 같은 인덱스를 서로 다른 transform에 적용\n","train_ds = Subset(train_aug, train_idx)  # 증강 O\n","val_ds   = Subset(val_eval,  val_idx)    # 증강 X\n","\n","# --------------------\n","# DataLoader\n","# --------------------\n","pin = torch.cuda.is_available()\n","num_workers = 0 # Changed from 2 to 0\n","\n","train_loader = DataLoader(\n","    train_ds, batch_size=32, shuffle=True, generator=g,\n","    num_workers=num_workers, pin_memory=pin, drop_last=True,\n","    persistent_workers=(num_workers > 0)\n",")\n","\n","val_loader = DataLoader(\n","    val_ds, batch_size=32, shuffle=False,\n","    num_workers=num_workers, pin_memory=pin,\n","    persistent_workers=(num_workers > 0)\n",")\n","\n","print(f\"train: {len(train_ds)} | val: {len(val_ds)}\")  # 40000 / 10000\n","classes = train_aug.classes\n","print(\"classes:\", classes)\n","print(\"device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iEIiL0wiFtkx","executionInfo":{"status":"ok","timestamp":1756143784993,"user_tz":-540,"elapsed":26053,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"b708f58e-03f6-4ba6-ef4f-7a8d456b03be"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:12<00:00, 13.4MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["train: 40000 | val: 10000\n","classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","device: cuda\n"]}]},{"cell_type":"markdown","source":["## 학습 모델\n","- VGG11-BN (ImageNet1K 사전학습 가중치)\n","- 입력 크기에 맞춰 avgpool / classifier 재구성\n","- Full Fine-Tuning"],"metadata":{"id":"3nAUz3o3f-Jy"}},{"cell_type":"code","source":["import torch, torch.nn as nn\n","from torchvision.models import vgg11_bn, VGG11_BN_Weights\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","num_classes = 10\n","IMG_SIZE = 128  # 128 해상도\n","\n","# 사전학습 모델 로드\n","vgg = vgg11_bn(weights=VGG11_BN_Weights.IMAGENET1K_V1)\n","\n","# 실제 feature map 크기 계산\n","with torch.no_grad():\n","    dummy = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE)\n","    c, h, w = vgg.features(dummy).shape[1:]   # c=512, h=w=4 (IMG_SIZE=128)\n","\n","assert h >= 1 and w >= 1, \"입력 해상도가 너무 작아요.\"\n","\n","# avgpool과 classifier 갱신\n","vgg.avgpool = nn.AdaptiveAvgPool2d((h, w))            # 안전하게 고정\n","vgg.classifier[0] = nn.Linear(c * h * w, 4096)        # in_features만 교체\n","vgg.classifier[6] = nn.Linear(4096, num_classes)      # 출력 차원 교체\n","\n","# Full fine-tuning\n","vgg.requires_grad_(True)  # 전체 학습\n","\n","vgg = vgg.to(device)"],"metadata":{"id":"0D6MqHoQIwcY","executionInfo":{"status":"ok","timestamp":1756143789148,"user_tz":-540,"elapsed":4144,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b4eda08-a26b-4a18-c6e7-2a65218f73d6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 507M/507M [00:02<00:00, 236MB/s]\n"]}]},{"cell_type":"code","source":["vgg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"7VwUoK9ZIyWK","executionInfo":{"status":"ok","timestamp":1756143789210,"user_tz":-540,"elapsed":34,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"222727f9-806c-442f-b9a0-a2af524f8709"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU(inplace=True)\n","    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU(inplace=True)\n","    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (13): ReLU(inplace=True)\n","    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (17): ReLU(inplace=True)\n","    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (20): ReLU(inplace=True)\n","    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (24): ReLU(inplace=True)\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (27): ReLU(inplace=True)\n","    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=8192, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## 기본 모델 생성"],"metadata":{"id":"PIXZgJ4kJHa6"}},{"cell_type":"code","source":["import os, torch, torch.nn as nn\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(vgg.parameters(), lr=0.005, momentum=0.9, weight_decay=5e-4)\n","\n","def train_one_epoch(model, loader):\n","    model.train()\n","    total, correct, run_loss = 0, 0, 0.0\n","    for x, y in loader:\n","        x = x.to(device, non_blocking=True)\n","        y = y.to(device, non_blocking=True)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","        out = model(x)\n","        loss = criterion(out, y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        run_loss += loss.item() * x.size(0)\n","        pred = out.argmax(1)\n","        correct += (pred == y).sum().item()\n","        total += y.size(0)\n","    return run_loss / total, correct / total\n","\n","@torch.no_grad()\n","def evaluate(model, loader):\n","    model.eval()\n","    total, correct, run_loss = 0, 0, 0.0\n","    for x, y in loader:\n","        x = x.to(device, non_blocking=True)\n","        y = y.to(device, non_blocking=True)\n","        out = model(x)\n","        loss = criterion(out, y)\n","\n","        run_loss += loss.item() * y.size(0)\n","        pred = out.argmax(1)\n","        correct += (pred == y).sum().item()\n","        total += y.size(0)\n","    return run_loss / total, correct / total"],"metadata":{"id":"ooKJ73LTbWm5","executionInfo":{"status":"ok","timestamp":1756143789222,"user_tz":-540,"elapsed":9,"user":{"displayName":"이대석","userId":"00034573553052925646"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 학습\n","epochs = 5\n","best = 0.0\n","for e in range(1, epochs + 1):\n","    tr_loss, tr_acc = train_one_epoch(vgg, train_loader)\n","    te_loss, te_acc = evaluate(vgg, val_loader)\n","\n","    if te_acc > best:\n","        best = te_acc\n","        torch.save(vgg.state_dict(), \"mission_16_vgg11bn_fp32.pth\")\n","\n","    print(f\"[Epoch {e:02d}] train loss {tr_loss:.4f} | acc {tr_acc*100:.2f}%  ||  \"\n","          f\"val loss {te_loss:.4f} | acc {te_acc*100:.2f}%\")\n","\n","print(f\"Best FP32 val acc: {best*100:.2f}% (saved to mission_16_vgg11bn_fp32.pth)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BDy9-bQAlGMf","executionInfo":{"status":"ok","timestamp":1756144328484,"user_tz":-540,"elapsed":539255,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"4bea9102-3f5c-4959-92e6-b9eb8cd69039"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 01] train loss 0.5679 | acc 80.54%  ||  val loss 0.3263 | acc 88.81%\n","[Epoch 02] train loss 0.2324 | acc 92.05%  ||  val loss 0.2300 | acc 92.44%\n","[Epoch 03] train loss 0.1571 | acc 94.73%  ||  val loss 0.2311 | acc 92.43%\n","[Epoch 04] train loss 0.1257 | acc 95.77%  ||  val loss 0.2362 | acc 92.43%\n","[Epoch 05] train loss 0.1009 | acc 96.63%  ||  val loss 0.2330 | acc 92.79%\n","Best FP32 val acc: 92.79% (saved to mission_16_vgg11bn_fp32.pth)\n"]}]},{"cell_type":"markdown","source":["## PTQ(Post-Training Quantization)\n","- 모델을 훈련 종료 후 , 가중치를변환\n","### Dynamic quantization\n","- 모델의 **Linear 층 가중치**를 미리 **INT8**로 압축해 둡니다\n","- 추론할 때 들어오는 **입력 활성값**은 그 순간 **on-the-fly로 INT8 변환**해서\n","    정수 곱셈으로 빠르게 계산하고, 결과는 다시 **FP32**로 되돌립니다."],"metadata":{"id":"61wThoIUeQUY"}},{"cell_type":"code","source":["# Dynamic Quantization (Linear만 INT8로 변환)\n","import copy, torch.nn as nn, os\n","\n","dq_model = torch.quantization.quantize_dynamic(\n","    copy.deepcopy(vgg).cpu().eval(),\n","    {nn.Linear},\n","    dtype=torch.qint8)\n","\n","# 저장\n","torch.save(dq_model.state_dict(), \"mission_16_vgg11bn_dynamic_int8.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o9OFEWjvUPDm","executionInfo":{"status":"ok","timestamp":1756144330310,"user_tz":-540,"elapsed":1830,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"4022fcb0-1d8d-4f1f-af4f-0481d4db95d3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3825890243.py:4: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n","For migrations of users: \n","1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n","2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n","3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n","see https://github.com/pytorch/ao/issues/2259 for more details\n","  dq_model = torch.quantization.quantize_dynamic(\n"]}]},{"cell_type":"markdown","source":["### Static quantization\n","- 정적 양자화에서는 사전 Calibration을 통해 가중치와 활성값의 분포를 미리 추정하고, 추론 시 가중치는 int8로, 활성값은 저장된 분포(scale/zero_point)를 사용해 int8로 변환한다."],"metadata":{"id":"Nlrv0dJdK7Da"}},{"cell_type":"code","source":["# --- Static Quantization (PTQ, INT8) - FX(Graph) 모드 / 128x128 입력\n","import copy, torch\n","from tqdm.auto import tqdm\n","from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n","from torch.ao.quantization import get_default_qconfig_mapping\n","\n","# 양자화 백엔진 선택 (x86 CPU → fbgemm / ARM → qnnpack)\n","torch.backends.quantized.engine = \"fbgemm\"\n","\n","# 원본 FP32 모델을 CPU+eval 로 복제\n","fp32_cpu = copy.deepcopy(vgg).to(\"cpu\", dtype=torch.float32).eval()\n","\n","# prepare_fx가 참고할 '양자화 설계도' 가져오기(Observer/스킴 배치 규칙, x86=FBGEMM)\n","qmap = get_default_qconfig_mapping(\"fbgemm\")\n","\n","# FX 트레이싱에 사용할 예제 입력\n","example_inputs = (torch.randn(1, 3, 128, 128),)\n","\n","# prepare_fx: 모델을 FX로 트레이싱하고, 각 텐서 경계에 Observer/Quant-DeQuant 스텁을 삽입\n","# - example_inputs로 그래프 캡처\n","# - Observer가 캘리브레이션 동안 활성/가중치 범위(min/max) 수집\n","# - 이 단계는 여전히 FP32로만 계산 (INT8 치환은 convert_fx에서 수행)\n","prepared = prepare_fx(fp32_cpu, qmap, example_inputs)\n","\n","# Calibration: 대표 배치 몇 개를 흘려보내 통계 수집 (CPU 실행)\n","MAX_CALIB_BATCHES = 20\n","prepared.eval()\n","with torch.inference_mode():\n","    for i, (x, _) in tqdm(enumerate(train_loader), total=min(MAX_CALIB_BATCHES, len(train_loader)),\n","                          desc=\"Calibrating (CPU)\", leave=False):\n","        prepared(x.cpu())\n","        if (i + 1) >= MAX_CALIB_BATCHES:\n","            break\n","\n","# convert_fx: 실제 INT8 연산 모듈로 치환 (정적 양자화 완료)\n","int8_static = convert_fx(prepared).eval()\n","\n","# 저장 (state_dict만 저장)\n","torch.save(int8_static.state_dict(), \"mission_16_vgg11bn_static_int8_fx.pth\")\n","print(\"saved: mission_vgg11bn_static_int8_fx.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":361,"referenced_widgets":["42bd0750cf844e389b40571ecee991a2","b317af5801b44f6187197d378e271031","d452fd33e5fe48b3a3d007c6b426500b","a4dbdae85fda4b21b09b70879dc6d18a","82d766883d014263936d46f77c2546a1","5fce2c5e1757480dacfeeae64cb4fb48","dbaa3c19e68e4401a8b6e5f26bf5093f","83c0970dce6a461eabcfb8cd619397d8","6b7e2d2077824285ad8b7969b1157f28","769b1129e98b48f29450031b0e07bae9","e496e257d1b748b99d79560e72ae7070"]},"id":"hv2TqRTYNdLn","executionInfo":{"status":"ok","timestamp":1756144346181,"user_tz":-540,"elapsed":15818,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"53d9a1c7-5811-47e2-8d41-ffee9c4b734b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1025594348.py:23: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n","For migrations of users: \n","1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n","2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n","3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n","see https://github.com/pytorch/ao/issues/2259 for more details\n","  prepared = prepare_fx(fp32_cpu, qmap, example_inputs)\n","/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Calibrating (CPU):   0%|          | 0/20 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42bd0750cf844e389b40571ecee991a2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1025594348.py:36: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n","For migrations of users: \n","1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n","2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n","3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n","see https://github.com/pytorch/ao/issues/2259 for more details\n","  int8_static = convert_fx(prepared).eval()\n"]},{"output_type":"stream","name":"stdout","text":["saved: mission_vgg11bn_static_int8_fx.pth\n"]}]},{"cell_type":"markdown","source":["## QAT\n","\n","forward에 **가짜 양자화(q→dq)**를 넣어 양자화 노이즈를 포함한 손실로 학습 →\n","그라디언트는 가중치를 업데이트해서 노이즈에 강건해지게 하고,\n","scale/zero_point는 observer가 런타임 분포를 보고 갱신(중간에 고정) → 최종 INT8 변환해도 정확도 유지.\n"],"metadata":{"id":"z5hg9L8IeV9y"}},{"cell_type":"code","source":["# =========================================================\n","# QAT (조건 일치: FP32와 동일 하이퍼/학습 흐름)\n","#   - LR=0.05, momentum=0.9, WD=5e-4 (FP32와 동일)\n","# =========================================================\n","\n","import copy\n","import torch\n","from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx\n","\n","# reduce_range 경고 방지용: quant_min/max를 명시한 QConfig 사용\n","from torch.ao.quantization.fake_quantize import FakeQuantize\n","from torch.ao.quantization.observer import MovingAverageMinMaxObserver, PerChannelMinMaxObserver\n","from torch.ao.quantization.qconfig import QConfig\n","from torch.ao.quantization.qconfig_mapping import QConfigMapping\n","\n","# -------------------------\n","# 0) 백엔드 & QConfig 구성\n","# -------------------------\n","torch.backends.quantized.engine = \"fbgemm\"  # x86 CPU 대상\n","\n","# activation: uint8(per-tensor-affine), weight: int8(per-channel-symmetric)\n","act_fake = FakeQuantize.with_args(\n","    observer=MovingAverageMinMaxObserver,\n","    dtype=torch.quint8, qscheme=torch.per_tensor_affine,\n","    quant_min=0, quant_max=255\n",")\n","wgt_fake = FakeQuantize.with_args(\n","    observer=PerChannelMinMaxObserver,\n","    dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0,\n","    quant_min=-128, quant_max=127\n",")\n","qmap_qat = QConfigMapping().set_global(QConfig(activation=act_fake, weight=wgt_fake))\n","\n","# -------------------------\n","# 1) QAT 준비 (FX graph mode)\n","#    - FP32 vgg를 복사 → prepare_qat_fx\n","# -------------------------\n","example_inputs = (torch.randn(1, 3, IMG_SIZE, IMG_SIZE),)\n","fp32_for_qat = copy.deepcopy(vgg).to(\"cpu\").train()\n","qat_model = prepare_qat_fx(fp32_for_qat, qmap_qat, example_inputs).to(device).train()\n","\n","# -------------------------\n","# 2) 학습 설정 (FP32와 동일)\n","# -------------------------\n","qat_optimizer = torch.optim.SGD(\n","    qat_model.parameters(), lr=0.005, momentum=0.9, weight_decay=5e-4\n",")\n","\n","def train_one_epoch_qat(model, loader, optimizer):\n","    \"\"\"FP32와 동일 형태의 학습 루프 (observer/BN 동결 없음)\"\"\"\n","    model.train()\n","    total = correct = 0\n","    run_loss = 0.0\n","    for x, y in loader:\n","        x = x.to(device, non_blocking=True)\n","        y = y.to(device, non_blocking=True)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","        out = model(x)\n","        loss = criterion(out, y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        run_loss += loss.item() * y.size(0)\n","        pred = out.argmax(1)\n","        correct += (pred == y).sum().item()\n","        total += y.size(0)\n","    return run_loss / total, correct / total\n","\n","# -------------------------\n","# 3) 학습 실행 (에폭 수는 FP32와 동일)\n","# -------------------------\n","EPOCHS = 5\n","best_qat = 0.0\n","for e in range(1, EPOCHS + 1):\n","    tr_loss, tr_acc = train_one_epoch_qat(qat_model, train_loader, qat_optimizer)\n","    va_loss, va_acc = evaluate(qat_model, val_loader)  # 기존 evaluate() 재사용 (device 평가)\n","\n","    if va_acc > best_qat:\n","        best_qat = va_acc\n","        torch.save(qat_model.state_dict(), \"mission_16_vgg11bn_qat_fake_fp32_match.pth\")\n","\n","    print(f\"[Epoch {e:02d}] train loss {tr_loss:.4f} | acc {tr_acc*100:.2f}%  ||  \"\n","          f\"val loss {va_loss:.4f} | acc {va_acc*100:.2f}%\")\n","\n","print(f\"Best QAT(fake-FP32) val acc: {best_qat*100:.2f}% \"\n","      f\"(saved to mission_16_vgg11bn_qat_fake_fp32_match.pth)\")\n","\n","# ---------------------------------------------------------\n","# 4)  INT8로 변환 후 CPU에서 평가/저장\n","#    - 배포 기준 정확도는 여기 값으로 판단\n","# ---------------------------------------------------------\n","@torch.no_grad()\n","def evaluate_int8_cpu(model_int8, loader):\n","    model_int8.eval()\n","    total = correct = 0\n","    run_loss = 0.0\n","    for x, y in loader:\n","        out  = model_int8(x.cpu())\n","        loss = criterion(out, y.cpu())\n","        run_loss += loss.item() * y.size(0)\n","        pred = out.argmax(1)\n","        correct += (pred.cpu() == y.cpu()).sum().item()\n","        total   += y.size(0)\n","    return run_loss/total, correct/total\n","\n","qat_cpu = copy.deepcopy(qat_model).to(\"cpu\").eval()\n","int8_qat = convert_fx(qat_cpu).eval()\n","val_loss_int8, val_acc_int8 = evaluate_int8_cpu(int8_qat, val_loader)\n","print(f\"[INT8] val loss {val_loss_int8:.4f} | acc {val_acc_int8*100:.2f}%\")\n","\n","torch.save(int8_qat.state_dict(), \"mission_16_vgg11bn_qat_int8_fp32match.pth\")\n","print(\"Saved INT8 state_dict → mission_16_vgg11bn_qat_int8_fp32match.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OfJ0iOufRj1m","executionInfo":{"status":"ok","timestamp":1756145302769,"user_tz":-540,"elapsed":956582,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"89b9ed67-fd20-44cd-bde0-808a5462ef49"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2282779019.py:42: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n","For migrations of users: \n","1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n","2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n","3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n","see https://github.com/pytorch/ao/issues/2259 for more details\n","  qat_model = prepare_qat_fx(fp32_for_qat, qmap_qat, example_inputs).to(device).train()\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 01] train loss 0.0803 | acc 97.24%  ||  val loss 0.2162 | acc 93.20%\n","[Epoch 02] train loss 0.0663 | acc 97.75%  ||  val loss 0.2706 | acc 92.24%\n","[Epoch 03] train loss 0.0661 | acc 97.75%  ||  val loss 0.2554 | acc 92.52%\n","[Epoch 04] train loss 0.0571 | acc 98.06%  ||  val loss 0.2551 | acc 92.52%\n","[Epoch 05] train loss 0.0526 | acc 98.19%  ||  val loss 0.2642 | acc 92.25%\n","Best QAT(fake-FP32) val acc: 93.20% (saved to mission_16_vgg11bn_qat_fake_fp32_match.pth)\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2282779019.py:110: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n","For migrations of users: \n","1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n","2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n","3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n","see https://github.com/pytorch/ao/issues/2259 for more details\n","  int8_qat = convert_fx(qat_cpu).eval()\n"]},{"output_type":"stream","name":"stdout","text":["[INT8] val loss 0.2651 | acc 92.26%\n","Saved INT8 state_dict → mission_16_vgg11bn_qat_int8_fp32match.pth\n"]}]},{"cell_type":"markdown","source":["## FP32 모델을 ONNX로 내보내기"],"metadata":{"id":"NSu4KGwVUToz"}},{"cell_type":"code","source":["!pip -q install onnx onnxruntime onnxruntime-tools"],"metadata":{"id":"On6GRuHEVb18","executionInfo":{"status":"ok","timestamp":1756145310879,"user_tz":-540,"elapsed":8095,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"79e01d67-e174-4ed7-cc8b-5697015c9c5e"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import torch, torch.nn as nn, torchvision as tv\n","\n","# 1) 가중치 로드\n","state = torch.load(\"mission_16_vgg11bn_fp32.pth\", map_location=\"cpu\")\n","\n","# 2) 학습 때와 같은 구조로 VGG 재구성 (128x128 → 4x4 → 8192)\n","model = tv.models.vgg11_bn(num_classes=10)\n","model.avgpool = nn.Identity()                 # ← 7x7로 강제 풀링을 끔(4x4 유지)\n","model.classifier[0] = nn.Linear(512*4*4, 4096)  # ← 첫 FC 입력을 8192로 교체\n","\n","# 3) 가중치 로드\n","model.load_state_dict(state)\n","model.eval().cpu()\n","\n","# 4) ONNX export (입력 128×128 기준)\n","dummy = torch.randn(1, 3, 128, 128)\n","torch.onnx.export(\n","    model, dummy, \"mission16_vgg11bn_fp32.onnx\",\n","    input_names=[\"input\"], output_names=[\"logits\"],\n","    dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n","    opset_version=13, do_constant_folding=True,\n",")\n","print(\"saved: mission_vgg11bn_fp32.onnx\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sy8zLpBWUTRo","executionInfo":{"status":"ok","timestamp":1756145315408,"user_tz":-540,"elapsed":4500,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"e555a920-b3f7-49cd-e334-9cab491d54ad"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1666021214.py:17: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n","  torch.onnx.export(\n"]},{"output_type":"stream","name":"stdout","text":["saved: mission_vgg11bn_fp32.onnx\n"]}]},{"cell_type":"markdown","source":["## ONNX를 INT8로 동적 양자화"],"metadata":{"id":"lXEhX76cSd4p"}},{"cell_type":"code","source":["from onnxruntime.quantization import quantize_dynamic, QuantType\n","\n","quantize_dynamic(\n","    model_input=\"mission16_vgg11bn_fp32.onnx\",\n","    model_output=\"mission16_vgg11bn_int8_dynamic.onnx\",\n","    weight_type=QuantType.QInt8,\n","    op_types_to_quantize=[\"MatMul\", \"Gemm\"],  # Linear(FC)만\n","    per_channel=False,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JTD9LJSTSdYN","executionInfo":{"status":"ok","timestamp":1756145321432,"user_tz":-540,"elapsed":5973,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"cc5a6f4a-e3ec-491f-db6f-6d14afc5590d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"]}]},{"cell_type":"markdown","source":["## 정적 양자화"],"metadata":{"id":"JGichapQWeEA"}},{"cell_type":"code","source":["# === ONNX → ORT 정적 양자화(PTQ, INT8) ===\n","# pip install onnx onnxruntime onnxruntime-tools\n","\n","import onnx, onnxruntime as ort\n","import torchvision as tv, torchvision.transforms as T\n","from torch.utils.data import DataLoader\n","from itertools import islice\n","from onnxruntime.quantization import (\n","    quantize_static, CalibrationDataReader,\n","    QuantType, CalibrationMethod, QuantFormat\n",")\n","\n","FP32_ONNX = \"mission16_vgg11bn_fp32.onnx\"\n","INT8_ONNX = \"mission16_vgg11bn_int8_static.onnx\"\n","IMG_SIZE  = 128\n","BATCH     = 32\n","MAX_BATCH = 40   # 32 * 40 = 1280장\n","\n","# shape inference로 메타정보 보강\n","onnx.save(onnx.shape_inference.infer_shapes(onnx.load(FP32_ONNX)), FP32_ONNX)\n","\n","# 전처리: 학습/검증과 동일\n","tf = T.Compose([\n","    T.Resize(IMG_SIZE),\n","    T.ToTensor(),\n","    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n","])\n","\n","# 캘리브레이션 데이터 , num_workers=0\n","calib_ds = tv.datasets.CIFAR10(\"./data\", train=False, download=True, transform=tf)\n","calib_ld = DataLoader(calib_ds, batch_size=BATCH, shuffle=False, num_workers=0)\n","\n","# CalibrationDataReader\n","class ORTCalib(CalibrationDataReader):\n","    def __init__(self, onnx_path, loader, max_batches):\n","        self.sess = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n","        self.name = self.sess.get_inputs()[0].name\n","        self.loader = loader\n","        self.max_batches = max_batches\n","        self._reset()\n","    def _reset(self):\n","        self._it = iter(islice(self.loader, self.max_batches))\n","    def get_next(self):\n","        try:\n","            x, _ = next(self._it)\n","            return {self.name: x.numpy()}  # float32 NCHW\n","        except StopIteration:\n","            return None\n","    def rewind(self):\n","        self._reset()\n","\n","dr = ORTCalib(FP32_ONNX, calib_ld, MAX_BATCH)\n","\n","# 정적 양자화 실행: Conv/Gemm(MatMul) 모두 INT8 대상\n","quantize_static(\n","    model_input=FP32_ONNX,\n","    model_output=INT8_ONNX,\n","    calibration_data_reader=dr,\n","    activation_type=QuantType.QUInt8,           # 활성값 UINT8\n","    weight_type=QuantType.QInt8,                # 가중치 INT8\n","    calibrate_method=CalibrationMethod.MinMax,  # 필요시 Entropy/Percentile로 바꿔 비교\n","    per_channel=True,                           # Conv 가중치 per-channel\n","    op_types_to_quantize=[\"Conv\",\"Gemm\",\"MatMul\"],\n","    quant_format=QuantFormat.QDQ,               # 최신 권장 포맷(QDQ)\n",")\n","\n","print(f\"saved: {INT8_ONNX}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWcSTvKeuJGf","executionInfo":{"status":"ok","timestamp":1756145355164,"user_tz":-540,"elapsed":33728,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"10287cbd-9548-40c0-93d4-e808ea54a100"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n","WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"]},{"output_type":"stream","name":"stdout","text":["saved: mission16_vgg11bn_int8_static.onnx\n"]}]},{"cell_type":"markdown","source":["## QAT UNNX 내보내기"],"metadata":{"id":"5_B3ZEjXX_md"}},{"cell_type":"code","source":["import torch, os\n","\n","onnx_path = \"mission_16_vgg11bn_qat_int8_fp32match.onnx\"\n","\n","# 배치 크기만 동적, 나머지는 고정\n","dynamic_axes = {\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}}\n","\n","# 더 안전한 내보내기를 위해 eval + CPU\n","model_export = int8_qat.to(\"cpu\").eval()\n","\n","# 더 과한 폴딩이 양자화 Q/DQ를 망치지 않게끔 비활성화 권장\n","dummy = torch.randn(1, 3, IMG_SIZE, IMG_SIZE, dtype=torch.float32)\n","\n","torch.onnx.export(\n","    model_export,\n","    dummy,\n","    onnx_path,\n","    export_params=True,\n","    opset_version=17,                      # ORT 최신 권장 (>=13)\n","    do_constant_folding=False,             # Q/DQ 패턴 보존\n","    input_names=[\"input\"],\n","    output_names=[\"logits\"],\n","    dynamic_axes=dynamic_axes,\n",")\n","\n","print(f\"[OK] Exported QAT INT8 model to ONNX → {os.path.abspath(onnx_path)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZxNm0VfW7-5","executionInfo":{"status":"ok","timestamp":1756145357346,"user_tz":-540,"elapsed":2176,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"b38d3a81-bf54-449a-f9e6-e3f951f0e31f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-240291950.py:14: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n","  torch.onnx.export(\n"]},{"output_type":"stream","name":"stdout","text":["[OK] Exported QAT INT8 model to ONNX → /content/mission_16_vgg11bn_qat_int8_fp32match.onnx\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"utP4aiGD17Wk"},"execution_count":null,"outputs":[]}]}