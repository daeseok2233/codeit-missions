{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMW5AyojC1fnsvL45JuJcf0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## inference data\n","- CIFAR 10  test 데이터\n","- test : 10000\n","- resize = 128"],"metadata":{"id":"D-uRJkou6eUs"}},{"cell_type":"code","metadata":{"collapsed":true,"id":"02d80d1f"},"source":["!pip install onnxruntime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8c84112c"},"source":["!pip install onnx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CPU"],"metadata":{"id":"zIrqIrRl5_0W"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YzyCuSO_-JjF","executionInfo":{"status":"ok","timestamp":1756148059437,"user_tz":-540,"elapsed":2094184,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"618faef2-57e4-4203-ec89-be2ca3739456"},"outputs":[{"output_type":"stream","name":"stdout","text":["ORT available providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:13<00:00, 12.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","=== mission16_vgg11bn_fp32.onnx ===\n","Providers(in use): ['CPUExecutionProvider']\n","size: 227.37 MB | acc: 92.28% | per_image: 63.161 ms | img/s: 15.8 | avg_batch: 2017.92 ms | batches: 313 | samples: 10000\n","\n","=== mission16_vgg11bn_int8_dynamic.onnx ===\n","Providers(in use): ['CPUExecutionProvider']\n","size: 83.25 MB | acc: 92.30% | per_image: 62.848 ms | img/s: 15.9 | avg_batch: 2007.92 ms | batches: 313 | samples: 10000\n","\n","=== mission16_vgg11bn_int8_static.onnx ===\n","Providers(in use): ['CPUExecutionProvider']\n","size: 57.03 MB | acc: 91.72% | per_image: 40.329 ms | img/s: 24.8 | avg_batch: 1288.47 ms | batches: 313 | samples: 10000\n","\n","=== mission_16_vgg11bn_qat_int8_fp32match.onnx ===\n","Providers(in use): ['CPUExecutionProvider']\n","size: 56.99 MB | acc: 91.31% | per_image: 37.867 ms | img/s: 26.4 | avg_batch: 1209.82 ms | batches: 313 | samples: 10000\n","\n","# 비교 요약 (size / speed / accuracy)\n","model                                    size(MB)    acc(%)   per_img(ms)     img/s          EP\n","-----------------------------------------------------------------------------------------------\n","mission16_vgg11bn_fp32.onnx                227.37     92.28        63.161      15.8  CPUExecutionProvider\n","mission16_vgg11bn_int8_dynamic.onnx         83.25     92.30        62.848      15.9  CPUExecutionProvider\n","mission16_vgg11bn_int8_static.onnx          57.03     91.72        40.329      24.8  CPUExecutionProvider\n","mission_16_vgg11bn_qat_int8_fp32match.onnx      56.99     91.31        37.867      26.4  CPUExecutionProvider\n","\n","Saved: onnx_benchmark_results.csv\n"]}],"source":["import os, time, math, csv\n","import numpy as np\n","import onnxruntime as ort\n","import torch\n","from torchvision import datasets, transforms\n","\n","# =========================\n","# Config\n","# =========================\n","MODEL_PATHS = [\n","    \"mission16_vgg11bn_fp32.onnx\",                 # FP32\n","    \"mission16_vgg11bn_int8_dynamic.onnx\",         # PTQ INT8 (dynamic)\n","    \"mission16_vgg11bn_int8_static.onnx\",          # PTQ INT8 (static)\n","    \"mission_16_vgg11bn_qat_int8_fp32match.onnx\",  # QAT INT8\n","]\n","\n","IMG_SIZE    = 128\n","BATCH_SIZE  = 32\n","NUM_WORKERS = max(2, (os.cpu_count() or 8) - 2)\n","PIN_MEMORY  = True\n","NORM_MEAN   = (0.4914, 0.4822, 0.4465)   # CIFAR-10\n","NORM_STD    = (0.2470, 0.2435, 0.2616)\n","WARMUP      = 3\n","CSV_OUT     = \"onnx_benchmark_results.csv\"\n","\n","# 동일 EP로 공정 비교를 원하면 \"CUDA\" 또는 \"CPU\"로 고정 (None은 GPU 우선/폴백)\n","FORCE_PROVIDER = None  # \"CUDA\" | \"CPU\" | None\n","\n","print(\"ORT available providers:\", ort.get_available_providers())\n","\n","# =========================\n","# DataLoader (128 고정 + Normalize)\n","# =========================\n","tfms = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BILINEAR),\n","    transforms.ToTensor(),\n","    transforms.Normalize(NORM_MEAN, NORM_STD),\n","])\n","test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfms)\n","test_loader = torch.utils.data.DataLoader(\n","    test_set, batch_size=BATCH_SIZE, shuffle=False,\n","    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",")\n","\n","# =========================\n","# ORT session helpers\n","# =========================\n","def build_session(onnx_path: str):\n","    so = ort.SessionOptions()\n","    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n","    so.intra_op_num_threads = max(1, os.cpu_count()//2)\n","\n","    if FORCE_PROVIDER == \"CUDA\":\n","        tries = [[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"], [\"CPUExecutionProvider\"]]\n","    elif FORCE_PROVIDER == \"CPU\":\n","        tries = [[\"CPUExecutionProvider\"]]\n","    else:\n","        if \"CUDAExecutionProvider\" in ort.get_available_providers():\n","            tries = [[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"], [\"CPUExecutionProvider\"]]\n","        else:\n","            tries = [[\"CPUExecutionProvider\"]]\n","\n","    last_err = None\n","    for providers in tries:\n","        try:\n","            sess = ort.InferenceSession(onnx_path, sess_options=so, providers=providers)\n","            inp = sess.get_inputs()[0].name\n","            out = sess.get_outputs()[0].name\n","            return sess, inp, out, providers\n","        except Exception as e:\n","            last_err = e\n","    raise RuntimeError(f\"Failed to create ORT session for {onnx_path}: {last_err}\")\n","\n","def onnx_predict_batch(sess, input_name, output_name, x: torch.Tensor) -> np.ndarray:\n","    x_np = np.ascontiguousarray(x.detach().cpu().numpy().astype(np.float32, copy=False))\n","    return sess.run([output_name], {input_name: x_np})[0]\n","\n","@torch.no_grad()\n","def evaluate_onnx(sess, input_name, output_name, loader, warmup=WARMUP):\n","    # warmup\n","    it = iter(loader)\n","    for _ in range(warmup):\n","        try:\n","            xb, _ = next(it)\n","        except StopIteration:\n","            break\n","        _ = onnx_predict_batch(sess, input_name, output_name, xb)\n","\n","    total, correct, times, n_batches = 0, 0, [], 0\n","    for xb, yb in loader:\n","        t0 = time.perf_counter()\n","        logits = onnx_predict_batch(sess, input_name, output_name, xb)\n","        t1 = time.perf_counter()\n","\n","        preds = logits.argmax(axis=1)\n","        y_np = yb.detach().cpu().numpy()\n","        correct += (preds == y_np).sum()\n","        total   += y_np.shape[0]\n","\n","        times.append(t1 - t0)\n","        n_batches += 1\n","\n","    acc = correct / total if total else 0.0\n","    avg_batch_ms = (np.mean(times) * 1000.0) if times else math.nan\n","    mean_bs = (total / n_batches) if n_batches else 0\n","    per_image_ms = (avg_batch_ms / mean_bs) if mean_bs else math.nan\n","    ips = (1000.0 / per_image_ms) if per_image_ms and per_image_ms > 0 else math.nan\n","    return acc, per_image_ms, ips, avg_batch_ms, n_batches, total\n","\n","def file_size_mb(path: str) -> float:\n","    try:\n","        return os.path.getsize(path) / (1024 * 1024)\n","    except OSError:\n","        return float(\"nan\")\n","\n","# =========================\n","# Run & collect\n","# =========================\n","rows = []\n","for path in MODEL_PATHS:\n","    if not os.path.exists(path):\n","        print(f\"[SKIP] {path} (파일 없음)\")\n","        continue\n","\n","    size_mb = file_size_mb(path)\n","    try:\n","        sess, in_name, out_name, providers = build_session(path)\n","    except Exception as e:\n","        print(f\"[FAIL] {path}: {e}\")\n","        continue\n","\n","    acc, per_img_ms, ips, avg_batch_ms, n_batches, total = evaluate_onnx(\n","        sess, in_name, out_name, test_loader\n","    )\n","\n","    print(f\"\\n=== {os.path.basename(path)} ===\")\n","    print(f\"Providers(in use): {providers}\")\n","    print(f\"size: {size_mb:.2f} MB | acc: {acc*100:.2f}% | per_image: {per_img_ms:.3f} ms | \"\n","          f\"img/s: {ips:.1f} | avg_batch: {avg_batch_ms:.2f} ms | batches: {n_batches} | samples: {total}\")\n","\n","    rows.append({\n","        \"model\": os.path.basename(path),\n","        \"size_mb\": round(size_mb, 2),\n","        \"acc_pct\": round(acc*100, 2),\n","        \"per_image_ms\": round(per_img_ms, 3),\n","        \"images_per_sec\": round(ips, 1),\n","        \"avg_batch_ms\": round(avg_batch_ms, 2),\n","        \"provider\": providers[0] if providers else \"unknown\",\n","    })\n","\n","# 요약 표\n","if rows:\n","    print(\"\\n# 비교 요약 (size / speed / accuracy)\")\n","    hdr = f\"{'model':38s}  {'size(MB)':>9s}  {'acc(%)':>8s}  {'per_img(ms)':>12s}  {'img/s':>8s}  {'EP':>10s}\"\n","    print(hdr)\n","    print(\"-\" * len(hdr))\n","    for r in rows:\n","        print(f\"{r['model']:38s}  {r['size_mb']:9.2f}  {r['acc_pct']:8.2f}  \"\n","              f\"{r['per_image_ms']:12.3f}  {r['images_per_sec']:8.1f}  {r['provider']:>10s}\")\n","\n","    # CSV 저장\n","    with open(CSV_OUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n","        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n","        writer.writeheader()\n","        writer.writerows(rows)\n","    print(f\"\\nSaved: {CSV_OUT}\")\n","else:\n","    print(\"평가할 ONNX 모델이 없습니다.\")"]},{"cell_type":"markdown","source":["## CUDA"],"metadata":{"id":"336vnQoP6CFA"}},{"cell_type":"code","source":["pip install -U onnxruntime-gpu"],"metadata":{"id":"J2fPTFqcRNVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# onnx_benchmark_gpu_only.py\n","# ------------------------------------------------------------\n","# pip uninstall -y onnxruntime\n","# pip install -U onnxruntime-gpu onnx torchvision torch\n","# ------------------------------------------------------------\n","import os, time, math, csv, json, gzip\n","import numpy as np\n","import onnxruntime as ort\n","import torch\n","from torchvision import datasets, transforms\n","\n","# =========================\n","# Config\n","# =========================\n","MODEL_PATHS = [\n","    \"mission16_vgg11bn_fp32.onnx\",                 # FP32\n","    \"mission16_vgg11bn_int8_dynamic.onnx\",         # PTQ INT8 (dynamic, FC-only이라고 가정)\n","    \"mission16_vgg11bn_int8_static.onnx\",          # PTQ INT8 (static)\n","    \"mission_16_vgg11bn_qat_int8_fp32match.onnx\",  # QAT INT8\n","]\n","\n","IMG_SIZE    = 128\n","BATCH_SIZE  = 32\n","NUM_WORKERS = max(2, (os.cpu_count() or 8) - 2)\n","PIN_MEMORY  = True\n","NORM_MEAN   = (0.4914, 0.4822, 0.4465)   # CIFAR-10\n","NORM_STD    = (0.2470, 0.2435, 0.2616)\n","WARMUP      = 3\n","CSV_OUT     = \"onnx_benchmark_results_gpu.csv\"\n","\n","# ---- GPU 강제 정책 ----\n","# CUDA만 허용(폴백 금지). TensorRT도 허용하려면 ALLOWED_EPS에 'TensorrtExecutionProvider'를 추가하세요.\n","ALLOWED_EPS = {\"CUDAExecutionProvider\"}\n","AVAILABLE_EPS = ort.get_available_providers()\n","print(\"ORT available providers:\", AVAILABLE_EPS)\n","assert \"CUDAExecutionProvider\" in AVAILABLE_EPS, \\\n","    \"CUDAExecutionProvider가 없습니다. 같은 가상환경에서 `pip install -U onnxruntime-gpu` 하세요.\"\n","\n","# =========================\n","# DataLoader\n","# =========================\n","tfms = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BILINEAR),\n","    transforms.ToTensor(),\n","    transforms.Normalize(NORM_MEAN, NORM_STD),\n","])\n","test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfms)\n","test_loader = torch.utils.data.DataLoader(\n","    test_set, batch_size=BATCH_SIZE, shuffle=False,\n","    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",")\n","\n","# =========================\n","# Helpers\n","# =========================\n","def build_session(onnx_path: str, device_id: int = 0):\n","    \"\"\"\n","    CUDA 전용 세션 생성. CPU/다른 EP는 등록 안 함.\n","    실제 실행 EP는 프로파일로 검증.\n","    \"\"\"\n","    so = ort.SessionOptions()\n","    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n","    so.intra_op_num_threads = max(1, os.cpu_count()//2)\n","    so.enable_profiling = True  # 실제 실행 EP 확인용\n","\n","    cuda_opts = {\"device_id\": device_id, \"arena_extend_strategy\": \"kNextPowerOfTwo\",\n","                 \"cudnn_conv_algo_search\": \"DEFAULT\", \"do_copy_in_default_stream\": True}\n","    providers = [(\"CUDAExecutionProvider\", cuda_opts)]\n","    # ⚠️ 여기서 CPU EP는 넣지 않습니다(폴백 금지).\n","    sess = ort.InferenceSession(onnx_path, sess_options=so, providers=providers)\n","    in_name  = sess.get_inputs()[0].name\n","    out_name = sess.get_outputs()[0].name\n","    return sess, in_name, out_name\n","\n","def _load_profile_any(path: str):\n","    \"\"\"ORT 프로파일 파일을 dict/list/JSON lines/gzip 모두 대응.\"\"\"\n","    with open(path, \"rb\") as f:\n","        raw = f.read()\n","    try:\n","        text = raw.decode(\"utf-8\")\n","    except UnicodeDecodeError:\n","        text = gzip.decompress(raw).decode(\"utf-8\")\n","    try:\n","        return json.loads(text)  # dict 또는 list\n","    except json.JSONDecodeError:\n","        items = []\n","        for line in text.splitlines():\n","            line = line.strip()\n","            if line.startswith(\"{\"):\n","                try:\n","                    items.append(json.loads(line))\n","                except Exception:\n","                    pass\n","        return items\n","\n","def collect_used_eps_from_profile(profile_path: str):\n","    \"\"\"\n","    프로파일에서 실제 실행된 EP 집계.\n","    반환: (used_eps_set, disallowed_eps_set)\n","    \"\"\"\n","    data = _load_profile_any(profile_path)\n","    if isinstance(data, dict):\n","        events = data.get(\"traceEvents\") or data.get(\"events\") or []\n","    elif isinstance(data, list):\n","        events = data\n","    else:\n","        events = []\n","\n","    used = set()\n","    for ev in events:\n","        if isinstance(ev, dict):\n","            args = ev.get(\"args\") or {}\n","            prov = args.get(\"provider\")\n","            if prov:\n","                used.add(prov)\n","\n","    # 간혹 provider가 안 찍히는 환경 → CUDA만 사용된 것으로 간주\n","    if not used:\n","        used = {\"CUDAExecutionProvider\"}\n","    bad = {p for p in used if p not in ALLOWED_EPS}\n","    return used, bad\n","\n","def onnx_predict_batch(sess, input_name, output_name, x: torch.Tensor) -> np.ndarray:\n","    # 입력은 CPU numpy로 전달 → ORT가 내부에서 GPU로 업로드하여 연산\n","    x_np = np.ascontiguousarray(x.detach().cpu().numpy().astype(np.float32, copy=False))\n","    return sess.run([output_name], {input_name: x_np})[0]\n","\n","@torch.no_grad()\n","def evaluate_onnx(sess, input_name, output_name, loader, warmup=WARMUP):\n","    # warmup\n","    it = iter(loader)\n","    for _ in range(warmup):\n","        try:\n","            xb, _ = next(it)\n","        except StopIteration:\n","            break\n","        _ = onnx_predict_batch(sess, input_name, output_name, xb)\n","\n","    total, correct, times, n_batches = 0, 0, [], 0\n","    for xb, yb in loader:\n","        t0 = time.perf_counter()\n","        logits = onnx_predict_batch(sess, input_name, output_name, xb)\n","        t1 = time.perf_counter()\n","\n","        preds = logits.argmax(axis=1)\n","        y_np = yb.detach().cpu().numpy()\n","        correct += (preds == y_np).sum()\n","        total   += y_np.shape[0]\n","\n","        times.append(t1 - t0)\n","        n_batches += 1\n","\n","    acc = correct / total if total else 0.0\n","    avg_batch_ms = (np.mean(times) * 1000.0) if times else math.nan\n","    mean_bs = (total / n_batches) if n_batches else 0\n","    per_image_ms = (avg_batch_ms / mean_bs) if mean_bs else math.nan\n","    ips = (1000.0 / per_image_ms) if per_image_ms and per_image_ms > 0 else math.nan\n","    return acc, per_image_ms, ips, avg_batch_ms, n_batches, total\n","\n","def file_size_mb(path: str) -> float:\n","    try:\n","        return os.path.getsize(path) / (1024 * 1024)\n","    except OSError:\n","        return float(\"nan\")\n","\n","# =========================\n","# Run & collect\n","# =========================\n","rows = []\n","for path in MODEL_PATHS:\n","    if not os.path.exists(path):\n","        print(f\"[SKIP] {path} (파일 없음)\")\n","        continue\n","\n","    size_mb = file_size_mb(path)\n","    try:\n","        sess, in_name, out_name = build_session(path)\n","    except Exception as e:\n","        print(f\"[FAIL] {path}: 세션 생성 실패 -> {e}\")\n","        continue\n","\n","    acc, per_img_ms, ips, avg_batch_ms, n_batches, total = evaluate_onnx(\n","        sess, in_name, out_name, test_loader\n","    )\n","\n","    # 실제 사용 EP 검증\n","    profile_path = sess.end_profiling()\n","    used_eps, bad_eps = collect_used_eps_from_profile(profile_path)\n","    if bad_eps:\n","        print(f\"[WARN] {os.path.basename(path)}: 비허용 EP 사용 감지 -> {sorted(bad_eps)} (all used: {sorted(used_eps)})\")\n","\n","    print(f\"\\n=== {os.path.basename(path)} ===\")\n","    print(f\"used EPs (from profile): {sorted(used_eps)}\")\n","    print(f\"size: {size_mb:.2f} MB | acc: {acc*100:.2f}% | per_image: {per_img_ms:.3f} ms | \"\n","          f\"img/s: {ips:.1f} | avg_batch: {avg_batch_ms:.2f} ms | batches: {n_batches} | samples: {total}\")\n","\n","    rows.append({\n","        \"model\": os.path.basename(path),\n","        \"size_mb\": round(size_mb, 2),\n","        \"acc_pct\": round(acc*100, 2),\n","        \"per_image_ms\": round(per_img_ms, 3),\n","        \"images_per_sec\": round(ips, 1),\n","        \"avg_batch_ms\": round(avg_batch_ms, 2),\n","        \"provider\": \"CUDAExecutionProvider\",\n","        \"used_eps\": \";\".join(sorted(used_eps)),\n","    })\n","\n","# 요약 표\n","if rows:\n","    print(\"\\n# 비교 요약 (size / speed / accuracy)\")\n","    hdr = f\"{'model':38s}  {'size(MB)':>9s}  {'acc(%)':>8s}  {'per_img(ms)':>12s}  {'img/s':>8s}  {'EP':>10s}\"\n","    print(hdr)\n","    print(\"-\" * len(hdr))\n","    for r in rows:\n","        print(f\"{r['model']:38s}  {r['size_mb']:9.2f}  {r['acc_pct']:8.2f}  \"\n","              f\"{r['per_image_ms']:12.3f}  {r['images_per_sec']:8.1f}  {'CUDA':>10s}\")\n","\n","    with open(CSV_OUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n","        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n","        writer.writeheader()\n","        writer.writerows(rows)\n","    print(f\"\\nSaved: {CSV_OUT}\")\n","else:\n","    print(\"평가할 ONNX 모델이 없습니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8wJPrUcRQcd","executionInfo":{"status":"ok","timestamp":1756150272542,"user_tz":-540,"elapsed":98839,"user":{"displayName":"이대석","userId":"00034573553052925646"}},"outputId":"4ce43d96-0957-4944-97f2-ea7c6eace1ba"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["ORT available providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n","\n","=== mission16_vgg11bn_fp32.onnx ===\n","used EPs (from profile): ['CUDAExecutionProvider']\n","size: 227.37 MB | acc: 92.28% | per_image: 1.937 ms | img/s: 516.4 | avg_batch: 61.87 ms | batches: 313 | samples: 10000\n","[WARN] mission16_vgg11bn_int8_dynamic.onnx: 비허용 EP 사용 감지 -> ['CPUExecutionProvider'] (all used: ['CPUExecutionProvider', 'CUDAExecutionProvider'])\n","\n","=== mission16_vgg11bn_int8_dynamic.onnx ===\n","used EPs (from profile): ['CPUExecutionProvider', 'CUDAExecutionProvider']\n","size: 83.25 MB | acc: 92.30% | per_image: 2.801 ms | img/s: 357.0 | avg_batch: 89.48 ms | batches: 313 | samples: 10000\n","[WARN] mission16_vgg11bn_int8_static.onnx: 비허용 EP 사용 감지 -> ['CPUExecutionProvider'] (all used: ['CPUExecutionProvider', 'CUDAExecutionProvider'])\n","\n","=== mission16_vgg11bn_int8_static.onnx ===\n","used EPs (from profile): ['CPUExecutionProvider', 'CUDAExecutionProvider']\n","size: 57.03 MB | acc: 92.24% | per_image: 2.291 ms | img/s: 436.4 | avg_batch: 73.21 ms | batches: 313 | samples: 10000\n","[WARN] mission_16_vgg11bn_qat_int8_fp32match.onnx: 비허용 EP 사용 감지 -> ['CPUExecutionProvider'] (all used: ['CPUExecutionProvider', 'CUDAExecutionProvider'])\n","\n","=== mission_16_vgg11bn_qat_int8_fp32match.onnx ===\n","used EPs (from profile): ['CPUExecutionProvider', 'CUDAExecutionProvider']\n","size: 56.99 MB | acc: 91.37% | per_image: 2.166 ms | img/s: 461.8 | avg_batch: 69.19 ms | batches: 313 | samples: 10000\n","\n","# 비교 요약 (size / speed / accuracy)\n","model                                    size(MB)    acc(%)   per_img(ms)     img/s          EP\n","-----------------------------------------------------------------------------------------------\n","mission16_vgg11bn_fp32.onnx                227.37     92.28         1.937     516.4        CUDA\n","mission16_vgg11bn_int8_dynamic.onnx         83.25     92.30         2.801     357.0        CUDA\n","mission16_vgg11bn_int8_static.onnx          57.03     92.24         2.291     436.4        CUDA\n","mission_16_vgg11bn_qat_int8_fp32match.onnx      56.99     91.37         2.166     461.8        CUDA\n","\n","Saved: onnx_benchmark_results_gpu.csv\n"]}]}]}